\documentclass[12pt,a4paper,DIV=calc]{scrartcl}
\usepackage[protrusion=true,expansion=true]{microtype}

\usepackage{float}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[bookmarks=true,hidelinks=true,breaklinks=true]{hyperref}
\usepackage{mathptmx}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage[backend=biber,natbib=true]{biblatex}
\usepackage[scaled=.92]{helvet}
\bibliography{references.bib}


\begin{document}
\title{Seminar paper on Probabilistic Near-Duplicate Detection Using Simhash}
\author{Arne Beer, MN 6489196, University of Hamburg}
\date{08.07.2019}

\maketitle


\section{Introduction}

In the age of the modern Internet, many services depend in large parts on crawlers and proper document detection and duplication elimination, near duplicate document detection becomes a necessity.
Real time detection of which website has already been visited and whether a website is new or just has been edited are important tasks during crawling~\cite{paper:scaling_six_billion}.

Standard hashing functions are often inefficient and operate in $O(n^2)$ space requirements for RAM and computing time~\cite{book:hashing}.
At the same time, the size of available documents grow steadily.
Google's website index alone has multiple hundreds of billions of web pages and over 100 Petabyte in size, according to their information website~\cite{info:google_stats}.

This paper attends to the paper \emph{Seminar paper on Probabilistic Near-Duplicate Detection Using Simhash}~\cite{inproc:main}.
The main challenge tackled by this paper is to find all matching pairs of fingerprints withing a certain Hamming distance $h$.
At this time, the fasted implementation for this procedure has been \emph{Block-Permuted Hamming Search} (BPHS), which requires RAM space at least four times the size of the whole dataset.
The authors of~\cite{inproc:main} aim to design a new algorithm that allows significantly faster online and batch document comparison and furthermore reduces RAM requirements, in exchange for a small percentage of recall loss.

In the first chapters, the basics for understanding this topic will be explained.
Afterwards the proposed algorithm will be looked at and the authors' findings will be discussed.

\subsection{Conventional Hashing}

Hashing is a technique, which is used to map data of an arbitrary size to a fingerprint with some fixed size.
This procedure could be seen as a function  $f(i) \rightarrow j$, which produces a value $j$ from from any value $i$, where $j \in H$ and $H$ is the set of values of the fixed length $s$ with $s \in \mathbb{N}$.
Well-known hash functions are, for instance, \emph{MD5} or \emph{SHA265}.
These hashing functions are commonly used to check whether two files are absolutely identical or, for instance, to verify that a file has not been corrupted during transport.
This is possible, since these hashing functions are designed to flip half of the output hash bits on average, if an input bit changes~\cite{book:hashing}.
Without this property it would be easier to change the input without the hash signature being modified.
This would allow malicious third parties to, for instance, change code in a binary, without users being able to detect the change with the help of this hash and would require a full byte level comparison between the original and the copied file to verify its integrity.

If, on the other hand, one's goal is to find near duplicates, which are identical for the most part, but sometimes only differ by a few bits or bytes, this hashing approach immediately becomes useless, due to this property.
Due to the need for a hashing algorithm, that creates a fingerprint based on the features and structure of the input data, \emph{simhash} has been created.


\section{Simhash}

To compare documents for similarity a methodology to create some uniform value, which should be identical for two similar matching documents.

\emph{simhash} is a procedure used to create a fingerprint of a any kind of data.
This fingerprint can then be used to, for instance, inspect two files for similarity.

The process for creating such a fingerprint can completely differ depending on the features in the hashed data one is interested in.
In case one wants to find similar binary files, it would be reasonable to split the data into equal chunks and use these chunks as features.
For websites or documents, looking at the composition and structure of text could be a viable approach to select features for hashing
The original data can then be refined to a high dimensional vector of features.

\begin{figure}[H]
\includegraphics[scale=0.5]{./gfx/binary_chunk_fingerprint}
\centering
\caption{Visual example for calculating a \emph{simhash} fingerprint.
A binary file split into chunks, which are then hashed.
Combining all hashes results in the desired fingerprint.~\cite{article:sampling-similarity}}\label{fig:simhash-example}
\end{figure}

The size of available features can vary significantly and is completely in the hand of the designer for each \emph{simhash} implementation.
It's important to note, that there exists no clear guideline on which features of a data set are interesting and which features can be ignored.
The performance of a \emph{simhash} implementation thereby also depends on the chosen features and the respective properties of the dataset.
Such features can be for instance binary chunks, file extension~\cite{inproc:simhash}, individual words, tags or URLs~\cite{inproc:main}.

After determining in which way feature are extracted from the original data, each feature is hashed and mapped onto a fingerprint, which represents the constellation of features.

\begin{figure}[h]
\includegraphics[scale=0.4]{./gfx/pseudocode.png}
\centering
\end{figure}

Algorithm 1 explains the process of~\cite{inproc:evaluation} for creating fingerprints.
At first one iterates over the features subset $F(u)$ of size $b$, which is selected from the set $F$ of all unique features with $F(u) \in F$.
Each feature is then uniformly hashed to a fix size.
This uniform hashing can be seen as a random projection of a arbitrarily large feature to a fixed size.
The amount of ones and zeros in this hash are then counted.
A one increments the counter, a zero decrements it.
The result is then stored in a weight vector $W$ under the respective index of the feature. $W$ has the same length $b$.

After calculating all values of $W$, a new binary Vector $B$ will then be created with the same Length as $W$.
Each value $v$ with $v_i \geq 0$ at index $i$ will result in $B[i] = 1$, while each $v_i < 0$ will result in $B[i] = 0$.

\subsection{Weights and Hamming Distance}

To achieve their goal of decreasing the runtime of fingerprint comparisons, the authors of~\emph{inproc:main} investigated the properties and distribution of weights of the temporary weight vector $W$ that was introduced the previous section.

For their current work, they used normalized term frequencyâ€“inverse document frequency (TF-IDF) weights.
This kind of weights is used to reflect the importance of a word to a document in a collection and is great for text-based document comparison methods~\cite{Salton:1986:IMI:576628}.
The corpus of the researchers consisted of 70M web pages.

\begin{figure}[H]
\includegraphics[scale=0.4]{./gfx/distribution.png}
\centering
\caption{Histogram of \emph{simhash} weights. A clear trend towards a Laplace-like distribution can be seen in a\).}\label{fig:weight-distribution}
\end{figure}

While using normalized TF-IDF weights, they noticed, that the distribution of weights rather resembles a Laplace-like distribution, than a Gaussian distribution, which has been expected, since the Central Limit theorem suggests it.
This can be seen in Figure~\ref{fig:weight-distribution}.

Using this knowledge they were able to generalize the formula for determining the specific weight of a feature to non-Bernoulli cases:

\[W_j(u) = \sum_{i \in F(u)} v_{ij}w_i(u)\]

$v$ is a vector, whose elements are drawn from a zero-mean symmetric distribution, which is theoretically equivalent to the Laplace-like distribution mentioned above.

After investigating the various probabilities and necessities for changing a weight to the side, it becomes apparent that certain bits are much more volatile to flip than others.
If one looks at the \emph{simhash} weights $(12, 0, -10)$, the second bit is much more likely to flip to the negative side, when small changes occur, than the other weights.
Much bigger changes would be needed to flip the weights at position 1 or 3.


\section{Bit order}

With the insight gained in the previous section, we are now able determine, which bits are more volatile to flip than others.
This knowledge can be utilized to determine an order, that should be used to determine the Hamming distance $h$ between the fingerprint and all hashes in the collection $D$.
While this is straight forward for single bit flips, where bits are simply ordered by their volatility, it becomes more complex, as soon as one wants to detect two-bit combination flips.

\subsection{Ordering}

$p_j(u)$ be the probability of another page in $D$, which has a flipped bit in position $j$ in the simhash of a page $u$ with $p_j(u) = P(Y_j > |W_j(u)|)$.
$Y$ is the random weight chance that occurs to changes in a document with $Y = X_j^{\alpha} - X_j^{\beta}$, where $X_j^{\alpha} - X_j^{\beta}$ are corresponding to the changed features.

Now it's possible to order and limit the Hamming search to the most likely subsets of hash bits.
For this let $S \subseteq \{1,2,\dots, b\}$ be a subset of non-empty hash bits.
The probability for the existence of a page in $D$ with differing bits on $S$ is then:

\[p(u,S) = \prod_{i \in S}p_i(u) \prod_{j \notin S}(1-p_j(u)) \]

Creating all possible subsets of $S$ in decreasing would be too time and space consuming, since it requires $\Theta(l \log l)$ operations and $\Theta(\sum_{i=1}^{h} ({b \atop i}))$ space.

\subsection{Volatility Heap}
To tackle the computation power and space requirement problems mentioned in the section above, the authors of~\cite{inproc:main} create a new data structure they call \emph{Volatility Ordered Set Heap} (VOSH).

W


\section{Impact in the scientific community}
This paper has been cited by 36 different publications.
Considering this paper has been released 2011, it's still cited by papers and books in 2019, which makes it still valuable contribution to the scientific community.

\section{Relevance as of 2019}
Near duplicate detection is still a very important research field.
The current trend shows, that data will continue to be generated in a exponential manner.

For instance, Google started using \emph{simhash} in 2007 for their web-crawling services~\cite{Manku:2007:DNW:1242572.1242592}.
With Google being one of the biggest Internet companies and the number one search engine, relevance of this topic is obviously very high.


\printbibliography
\end{document}
